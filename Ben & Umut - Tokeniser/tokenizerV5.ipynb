{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63fb69b7-f9a2-4847-8288-cc05ea811cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/usa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import psycopg2\n",
    "import re\n",
    "import nltk\n",
    "from urllib.parse import urlparse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05e9fbc0-b7bb-4554-af45-163ffa6e3f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Hacker News data...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Load and preprocess Hacker News data with URLs\n",
    "# -----------------------------\n",
    "print(\"Fetching Hacker News data...\")\n",
    "conn = psycopg2.connect(\"postgres://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT title, url, score FROM hacker_news.items WHERE title IS NOT NULL AND score IS NOT NULL LIMIT 10000;\")\n",
    "data = cur.fetchall()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5cb2bb5-f9b6-4fe4-b275-168fb87246ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Extract and process data\n",
    "tokenized_titles = [preprocess(title) for title, _, _ in data]\n",
    "upvotes = [float(up) for _, _, up in data]\n",
    "urls = [url for _, url, _ in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699b4a1e-f6e3-41f3-a8a0-f71c693ccaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 18.0, 1.0, 3.0, 8.0, 5.0, 2.0, 1.0, 3.0, 1.0, 9.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(upvotes[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec45851-1b17-4f76-b065-fab0a13ca2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['android', 'chrome', 'update', 'brings', 'stability', 'and', 'performance', 'improvements'], ['lifelikes', 'religious', 'extremists', 'or', 'marketing', 'through', 'intellectual', 'stimulus'], ['3d', 'printer', 'on', 'moon', 'or', 'mars', 'could', 'make', 'tools', 'from', 'local', 'rocks'], ['the', 'bookstore', 'strikes', 'back'], ['a', 'new', 'model', 'for', 'the', 'musiciantolistener', 'relationship'], ['palestines', 'bid', 'to', 'statehood'], ['learn', 'more', 'by', 'asking', 'fewer', 'questions'], ['latest', 'trend', 'in', 'piling', 'builidngs', 'ontop', 'of', 'each', 'other'], ['ask', 'hn', 'how', 'much', 'money', 'does', 'your', 'ios', 'app', 'make'], ['the', 'security', 'mindset', 'engineers', 'vs', 'security', 'professionals'], ['att', 'ceo', 'says', 'hard', 'to', 'find', 'skilled', 'us', 'workers'], ['fear', 'of', 'housing', 'slump', 'may', 'be', 'seriously', 'overdone'], ['rubiks', 'cube', 'proof', 'cut', 'to', '25', 'moves'], ['acid3', 'was', 'broken'], ['use', 'case', 'scaling', 'social', 'science', 'with', 'hadoop'], ['tipped', 'the', 'javascript', 'tooltip', 'framework'], ['dear', 'twitter', 'youre', 'a', 'utility', 'get', 'off', 'the', 'fence', 'start', 'acting', 'like', 'one'], ['despite', '8615', 'percent', 'growth', 'android', 'market', 'revenues', 'remain', 'puny'], ['kevin', 'kelly', 'a', 'jolt', 'to', 'the', 'soul', 'exploring', 'asia', 'with', 'a', 'camera', 'and', 'no', 'money'], ['foursquare', 'the', 'coolest', 'startup', 'office', 'weve', 'ever', 'seen']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_titles[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cedefb4-d3e3-458d-b891-05f303ca8e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://technobb.com/google/android-chrome-update-brings-stability-and-performance-improvements/', 'http://www.watchlifelike.com/index.php/lifelikes-religious-extremists-or-marketing-through-intellectual-stimulus/', 'http://news.cnet.com/8301-17938_105-57556017-1/3d-printer-on-moon-or-mars-could-make-tools-from-local-rocks/', 'http://www.theatlantic.com/magazine/archive/2012/12/the-bookstore-strikes-back/309164/?single_page=true', 'https://medium.com/the-future-of-publishing/44e778827320', 'http://www.avaaz.org/en/palestine_worlds_next_nation_a/?fp', 'http://blogs.hbr.org/schrage/2012/08/learn-more-by-asking-fewer-questions.html', 'http://www.dezeen.com/2012/11/19/peruri-88-by-mvrdvthe-jerde-partnership-and-arup-dublin/', None, 'http://www.schneier.com/blog/archives/2008/03/the_security_mi_1.html', 'http://news.yahoo.com/s/nm/20080327/tc_nm/att_workforce_dc', 'http://online.wsj.com/article/SB120640528180260969.html?mod=googlenews_wsj', 'http://arxivblog.com/?p=332', 'http://ln.hixie.ch/?start=1206578003&count=1', 'http://www.cloudera.com/blog/2010/04/scaling-social-science-with-hadoop/', 'http://projects.nickstakenburg.com/tipped', 'http://www.readwriteweb.com/archives/dear_twitter_youre_a_utility_-_get_off_the_fence_and_start_acting_like_one.php', 'http://techcrunch.com/2011/02/21/861-5-percent-growth-android-puny/', 'http://travelhappy.info/travel-books/kevin-kelly-interview-a-jolt-to-the-soul-the-making-of-asia-grace/', 'http://www.businessinsider.com/foursquare-slideshow-2012-8?op=1']\n"
     ]
    }
   ],
   "source": [
    "print(urls[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fcd3301-0bd8-4408-bc99-c79fab3cbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract domain features from URLs\n",
    "def extract_domain_features(url_list):\n",
    "    domains = []\n",
    "    domain_lengths = []\n",
    "    is_https = []\n",
    "    \n",
    "    for url in url_list:\n",
    "        if not url or not isinstance(url, str):  # Handle missing/None URLs\n",
    "            domains.append('unknown')\n",
    "            domain_lengths.append(0)\n",
    "            is_https.append(0)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            domain = parsed.netloc\n",
    "            domains.append(domain)\n",
    "            domain_lengths.append(len(domain))\n",
    "            is_https.append(1 if parsed.scheme == 'https' else 0)\n",
    "        except:\n",
    "            domains.append('unknown')\n",
    "            domain_lengths.append(0)\n",
    "            is_https.append(0)\n",
    "    \n",
    "    # Encode domains numerically\n",
    "    le = LabelEncoder()\n",
    "    domain_encoded = le.fit_transform(domains)\n",
    "    \n",
    "    return np.column_stack([\n",
    "        domain_encoded,\n",
    "        domain_lengths,\n",
    "        is_https\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6f8d110-f259-4340-b847-cb346552a637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2301\n"
     ]
    }
   ],
   "source": [
    "# Get URL features\n",
    "url_features = extract_domain_features(urls)\n",
    "url_features = torch.tensor(url_features, dtype=torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Build Vocabulary\n",
    "# -----------------------------\n",
    "from collections import Counter\n",
    "all_tokens = [token for title in tokenized_titles for token in title]\n",
    "vocab = [word for word, freq in Counter(all_tokens).items() if freq >= 5]\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aa4cf1f-0fbc-40b0-b5f2-f51cc44cfa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['android', 'chrome', 'update', 'brings', 'and', 'performance', 'or', 'marketing', 'through', 'intellectual', '3d', 'printer', 'on', 'moon', 'mars', 'could', 'make', 'tools', 'from', 'local']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a75173-edde-46c6-a202-6d752a6145b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CBOW pairs: 19077\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 3. Prepare CBOW training data\n",
    "# -----------------------------\n",
    "window_size = 2\n",
    "cbow_data = []\n",
    "for title in tokenized_titles:\n",
    "    indexed = [word_to_ix[word] for word in title if word in word_to_ix]\n",
    "    for i in range(window_size, len(indexed) - window_size):\n",
    "        context = indexed[i - window_size:i] + indexed[i + 1:i + window_size + 1]\n",
    "        target = indexed[i]\n",
    "        cbow_data.append((context, target))\n",
    "print(f\"Training CBOW pairs: {len(cbow_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3383b3e4-84ef-498e-a43a-16338cb13406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. CBOW Model\n",
    "# -----------------------------\n",
    "embedding_dim = 100\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    def forward(self, context_idxs):\n",
    "        embeds = self.embeddings(context_idxs)\n",
    "        avg_embed = embeds.mean(dim=1)\n",
    "        out = self.linear(avg_embed)\n",
    "        return out\n",
    "\n",
    "cbow_model = CBOW(vocab_size, embedding_dim)\n",
    "cbow_loss_fn = nn.CrossEntropyLoss()\n",
    "cbow_optimizer = optim.Adam(cbow_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ba5d1a6-923d-4489-a4e4-7a6f18cea797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CBOW model on HN titles...\n",
      "Epoch 1, CBOW Loss: 127278.03\n",
      "Epoch 2, CBOW Loss: 111313.49\n",
      "Epoch 3, CBOW Loss: 104768.17\n",
      "Epoch 4, CBOW Loss: 98382.85\n",
      "Epoch 5, CBOW Loss: 92343.16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 5. Train CBOW Model\n",
    "# -----------------------------\n",
    "print(\"Training CBOW model on HN titles...\")\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for context, target in cbow_data:\n",
    "        context_var = torch.tensor([context], dtype=torch.long)\n",
    "        target_var = torch.tensor([target], dtype=torch.long)\n",
    "        cbow_model.zero_grad()\n",
    "        logits = cbow_model(context_var)\n",
    "        loss = cbow_loss_fn(logits, target_var)\n",
    "        loss.backward()\n",
    "        cbow_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, CBOW Loss: {total_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5aae03b-1f7f-40f6-85f5-94c501b9005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Create averaged title embeddings and combine with URL features\n",
    "# -----------------------------\n",
    "title_embeddings = []\n",
    "valid_labels = []\n",
    "valid_indices = []  # To keep track of which samples we're keeping\n",
    "\n",
    "for idx, (tokens, label) in enumerate(zip(tokenized_titles, upvotes)):\n",
    "    token_ids = [word_to_ix[t] for t in tokens if t in word_to_ix]\n",
    "    if token_ids:\n",
    "        with torch.no_grad():\n",
    "            vectors = cbow_model.embeddings(torch.tensor(token_ids))\n",
    "            avg_vector = vectors.mean(dim=0)\n",
    "        title_embeddings.append(avg_vector)\n",
    "        valid_labels.append(label)\n",
    "        valid_indices.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "563b4804-e998-481a-af31-c81ee12a8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack embeddings and get corresponding URL features\n",
    "X_title = torch.stack(title_embeddings)\n",
    "X_url = url_features[valid_indices]  # Only keep URL features for valid samples\n",
    "y = torch.tensor(valid_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Combine title embeddings and URL features\n",
    "X_combined = torch.cat([X_title, X_url], dim=1)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Dataset and Dataloader\n",
    "# -----------------------------\n",
    "class HNDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = HNDataset(X_combined, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccf4ef31-deba-471f-967b-24eb3030ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8. Enhanced Regression Model (now with URL features)\n",
    "# -----------------------------\n",
    "class UpvotePredictor(nn.Module):\n",
    "    def __init__(self, title_embed_dim, url_feat_dim):\n",
    "        super().__init__()\n",
    "        combined_dim = title_embed_dim + url_feat_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = UpvotePredictor(embedding_dim, url_features.shape[1])\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5936dbb9-b32d-4159-97df-ee8400e46b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training enhanced upvote regression model...\n",
      "Epoch 1, Loss: 530761.3510\n",
      "Epoch 2, Loss: 526978.8422\n",
      "Epoch 3, Loss: 524142.1476\n",
      "Epoch 4, Loss: 522481.2534\n",
      "Epoch 5, Loss: 522720.1001\n",
      "Epoch 6, Loss: 521357.9524\n",
      "Epoch 7, Loss: 521041.1453\n",
      "Epoch 8, Loss: 520943.0590\n",
      "Epoch 9, Loss: 517661.1490\n",
      "Epoch 10, Loss: 516180.8094\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9. Train Enhanced Regression Model\n",
    "# -----------------------------\n",
    "print(\"Training enhanced upvote regression model...\")\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        pred = model(batch_x)\n",
    "        loss = loss_fn(pred, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e63f8ce7-3340-4ccf-af4b-bc823826942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_upvotes(title, url):\n",
    "    # Preprocess and tokenize the title\n",
    "    tokens = preprocess(title)\n",
    "    token_ids = [word_to_ix[t] for t in tokens if t in word_to_ix]\n",
    "    if not token_ids:\n",
    "        print(\"No known tokens in title.\")\n",
    "        return None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        title_vecs = cbow_model.embeddings(torch.tensor(token_ids))\n",
    "        avg_embed = title_vecs.mean(dim=0)\n",
    "\n",
    "    # Extract URL features\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc or \"unknown\"\n",
    "    domain_len = len(domain)\n",
    "    https_flag = 1 if parsed.scheme == 'https' else 0\n",
    "    try:\n",
    "        domain_encoded = le.transform([domain])[0]\n",
    "    except:\n",
    "        domain_encoded = 0  # Handle unseen domain\n",
    "\n",
    "    url_feat = torch.tensor([[domain_encoded, domain_len, https_flag]], dtype=torch.float32)\n",
    "\n",
    "    # Combine title embedding and URL features\n",
    "    x_combined = torch.cat([avg_embed.unsqueeze(0), url_feat], dim=1)\n",
    "\n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(x_combined).item()\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fffe67ca-eee8-47bf-94fc-be80de7bb4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted upvotes for \"AI Hacker Cracks the Code\" → 36.95\n"
     ]
    }
   ],
   "source": [
    "# Sample prediction\n",
    "title_input = \"AI Hacker Cracks the Code\"\n",
    "url_input = \"https://google.com\"\n",
    "predicted_score = predict_upvotes(title_input, url_input)\n",
    "print(f\"Predicted upvotes for \\\"{title_input}\\\" → {predicted_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d364ea4-1943-452c-a286-73eb008527ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
