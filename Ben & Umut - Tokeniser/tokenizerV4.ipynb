{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63fb69b7-f9a2-4847-8288-cc05ea811cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/usa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import psycopg2\n",
    "import re\n",
    "import nltk\n",
    "from urllib.parse import urlparse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05e9fbc0-b7bb-4554-af45-163ffa6e3f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Hacker News data...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Load and preprocess Hacker News data with URLs\n",
    "# -----------------------------\n",
    "print(\"Fetching Hacker News data...\")\n",
    "conn = psycopg2.connect(\"postgres://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT title, url, score FROM hacker_news.items WHERE title IS NOT NULL AND score IS NOT NULL LIMIT 10000;\")\n",
    "data = cur.fetchall()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5cb2bb5-f9b6-4fe4-b275-168fb87246ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Extract and process data\n",
    "tokenized_titles = [preprocess(title) for title, _, _ in data]\n",
    "upvotes = [float(up) for _, _, up in data]\n",
    "urls = [url for _, url, _ in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699b4a1e-f6e3-41f3-a8a0-f71c693ccaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144.0, 1.0, 3.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 5.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 180.0]\n"
     ]
    }
   ],
   "source": [
    "print(upvotes[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eec45851-1b17-4f76-b065-fab0a13ca2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['breaking', 'textbook', 'rsa', 'used', 'to', 'protect', 'the', 'privacy', 'of', 'millions', 'of', 'users'], ['evernote', 'is', 'in', 'deep', 'trouble'], ['found', 'a', '90s', 'chatroom', 'for', 'horses', 'on', 'a', 'bunch', 'of', 'old', 'floppy', 'disks'], ['theorytheory'], ['ask', 'hn', 'what', 'are', 'the', 'best', 'book', 'on', 'startup'], ['is', 'there', 'a', 'me', 'inside', 'of', 'here', 'that', 'is', 'dying', 'to', 'get', 'out'], ['political', 'correctness', 'is', 'stifling', 'innovation'], ['semisupervised', 'image', 'classification', 'explained'], ['earthday2016whattodoandwear'], ['chinas', 'next', 'target', 'us', 'microchip', 'hegemony'], ['graphql', 'at', 'braintree'], ['this', 'is', 'americas', 'richest', 'zip', 'code'], ['angular', '6', 'universal', 'pwa', 'installable', 'from', 'npm'], ['goes', 'satellite', 'view', 'of', 'eclipse'], ['shaaaaaaaaaaaaa'], ['here', 'be', 'dragons', 'the', 'mythic', 'bite', 'of', 'the', 'komodo', 'science', 'sushi'], ['how', 'the', 'mig31', 'repelled', 'the', 'sr71', 'blackbird', 'from', 'soviet', 'skies'], ['request', 'for', 'nsa', 'records', 'relating', 'to', 'colin', 'gerard', 'gallagher'], ['the', 'times', 'and', 'the', 'sunday', 'times', 'are', 'now', 'https'], ['saltpack', 'a', 'modern', 'crypto', 'messaging', 'format']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_titles[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cedefb4-d3e3-458d-b891-05f303ca8e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://arxiv.org/abs/1802.03367', 'http://businessinsider.com/evernote-is-in-deep-trouble-2015-10', 'http://www.deathandtaxesmag.com/265340/horsechat-horsechat-horsechat-horsechat/', 'https://en.wikipedia.org/wiki/Theory-theory', None, 'http://uber.la/2016/10/inside-dying-get/', 'http://www.breitbart.com/california/2017/12/19/silicon-valley-political-correctness-stifling-innovation/', 'https://thecuriousaicompany.com/mean-teacher/', 'http://claudettepesterine.com/2016/04/earth-day-2016-wear/?utm_campaign=shareaholic&utm_medium=yc_hacker_news&utm_source=news', 'https://www.wsj.com/articles/chinas-next-target-u-s-microchip-hegemony-1501168303', 'https://www.braintreepayments.com/blog/graphql-at-braintree/', 'https://www.bloomberg.com/news/articles/2018-04-10/to-visit-america-s-richest-zip-code-first-you-ll-need-a-boat', None, 'http://www.goes.noaa.gov/GSSLOOPS/wcvs.html', 'https://shaaaaaaaaaaaaa.com/', 'http://blogs.discovermagazine.com/science-sushi/2013/06/25/here-be-dragons-the-mythic-bite-of-the-komodo/', 'https://theaviationist.com/2013/12/11/sr-71-vs-mig-31/', 'https://www.muckrock.com/foi/united-states-of-america-10/request-for-nsa-records-relating-to-colin-gerard-gallagher-18277/', 'https://medium.com/digital-times/the-times-and-the-sunday-times-are-now-https-78963f7dd46f', 'https://saltpack.org/']\n"
     ]
    }
   ],
   "source": [
    "print(urls[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fcd3301-0bd8-4408-bc99-c79fab3cbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract domain features from URLs\n",
    "def extract_domain_features(url_list):\n",
    "    domains = []\n",
    "    domain_lengths = []\n",
    "    is_https = []\n",
    "    \n",
    "    for url in url_list:\n",
    "        if not url or not isinstance(url, str):  # Handle missing/None URLs\n",
    "            domains.append('unknown')\n",
    "            domain_lengths.append(0)\n",
    "            is_https.append(0)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            domain = parsed.netloc\n",
    "            domains.append(domain)\n",
    "            domain_lengths.append(len(domain))\n",
    "            is_https.append(1 if parsed.scheme == 'https' else 0)\n",
    "        except:\n",
    "            domains.append('unknown')\n",
    "            domain_lengths.append(0)\n",
    "            is_https.append(0)\n",
    "    \n",
    "    # Encode domains numerically\n",
    "    le = LabelEncoder()\n",
    "    domain_encoded = le.fit_transform(domains)\n",
    "    \n",
    "    return np.column_stack([\n",
    "        domain_encoded,\n",
    "        domain_lengths,\n",
    "        is_https\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f8d110-f259-4340-b847-cb346552a637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2489\n"
     ]
    }
   ],
   "source": [
    "# Get URL features\n",
    "url_features = extract_domain_features(urls)\n",
    "url_features = torch.tensor(url_features, dtype=torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Build Vocabulary\n",
    "# -----------------------------\n",
    "from collections import Counter\n",
    "all_tokens = [token for title in tokenized_titles for token in title]\n",
    "vocab = [word for word, freq in Counter(all_tokens).items() if freq >= 5]\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa4cf1f-0fbc-40b0-b5f2-f51cc44cfa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['breaking', 'used', 'to', 'protect', 'the', 'privacy', 'of', 'millions', 'users', 'is', 'in', 'deep', 'trouble', 'found', 'a', 'for', 'on', 'old', 'ask', 'hn']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92a75173-edde-46c6-a202-6d752a6145b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CBOW pairs: 24450\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 3. Prepare CBOW training data\n",
    "# -----------------------------\n",
    "window_size = 2\n",
    "cbow_data = []\n",
    "for title in tokenized_titles:\n",
    "    indexed = [word_to_ix[word] for word in title if word in word_to_ix]\n",
    "    for i in range(window_size, len(indexed) - window_size):\n",
    "        context = indexed[i - window_size:i] + indexed[i + 1:i + window_size + 1]\n",
    "        target = indexed[i]\n",
    "        cbow_data.append((context, target))\n",
    "print(f\"Training CBOW pairs: {len(cbow_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3383b3e4-84ef-498e-a43a-16338cb13406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. CBOW Model\n",
    "# -----------------------------\n",
    "embedding_dim = 100\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    def forward(self, context_idxs):\n",
    "        embeds = self.embeddings(context_idxs)\n",
    "        avg_embed = embeds.mean(dim=1)\n",
    "        out = self.linear(avg_embed)\n",
    "        return out\n",
    "\n",
    "cbow_model = CBOW(vocab_size, embedding_dim)\n",
    "cbow_loss_fn = nn.CrossEntropyLoss()\n",
    "cbow_optimizer = optim.Adam(cbow_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ba5d1a6-923d-4489-a4e4-7a6f18cea797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CBOW model on HN titles...\n",
      "Epoch 1, CBOW Loss: 163973.86\n",
      "Epoch 2, CBOW Loss: 147424.31\n",
      "Epoch 3, CBOW Loss: 139970.70\n",
      "Epoch 4, CBOW Loss: 132397.04\n",
      "Epoch 5, CBOW Loss: 126074.64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 5. Train CBOW Model\n",
    "# -----------------------------\n",
    "print(\"Training CBOW model on HN titles...\")\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for context, target in cbow_data:\n",
    "        context_var = torch.tensor([context], dtype=torch.long)\n",
    "        target_var = torch.tensor([target], dtype=torch.long)\n",
    "        cbow_model.zero_grad()\n",
    "        logits = cbow_model(context_var)\n",
    "        loss = cbow_loss_fn(logits, target_var)\n",
    "        loss.backward()\n",
    "        cbow_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, CBOW Loss: {total_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5aae03b-1f7f-40f6-85f5-94c501b9005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Create averaged title embeddings and combine with URL features\n",
    "# -----------------------------\n",
    "title_embeddings = []\n",
    "valid_labels = []\n",
    "valid_indices = []  # To keep track of which samples we're keeping\n",
    "\n",
    "for idx, (tokens, label) in enumerate(zip(tokenized_titles, upvotes)):\n",
    "    token_ids = [word_to_ix[t] for t in tokens if t in word_to_ix]\n",
    "    if token_ids:\n",
    "        with torch.no_grad():\n",
    "            vectors = cbow_model.embeddings(torch.tensor(token_ids))\n",
    "            avg_vector = vectors.mean(dim=0)\n",
    "        title_embeddings.append(avg_vector)\n",
    "        valid_labels.append(label)\n",
    "        valid_indices.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "563b4804-e998-481a-af31-c81ee12a8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack embeddings and get corresponding URL features\n",
    "X_title = torch.stack(title_embeddings)\n",
    "X_url = url_features[valid_indices]  # Only keep URL features for valid samples\n",
    "y = torch.tensor(valid_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Combine title embeddings and URL features\n",
    "X_combined = torch.cat([X_title, X_url], dim=1)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Dataset and Dataloader\n",
    "# -----------------------------\n",
    "class HNDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = HNDataset(X_combined, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccf4ef31-deba-471f-967b-24eb3030ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8. Enhanced Regression Model (now with URL features)\n",
    "# -----------------------------\n",
    "class UpvotePredictor(nn.Module):\n",
    "    def __init__(self, title_embed_dim, url_feat_dim):\n",
    "        super().__init__()\n",
    "        combined_dim = title_embed_dim + url_feat_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = UpvotePredictor(embedding_dim, url_features.shape[1])\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5936dbb9-b32d-4159-97df-ee8400e46b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training enhanced upvote regression model...\n",
      "Epoch 1, Loss: 942215.6909\n",
      "Epoch 2, Loss: 935179.0526\n",
      "Epoch 3, Loss: 934054.3995\n",
      "Epoch 4, Loss: 930922.0938\n",
      "Epoch 5, Loss: 929337.5126\n",
      "Epoch 6, Loss: 929907.0703\n",
      "Epoch 7, Loss: 927330.7702\n",
      "Epoch 8, Loss: 924687.2913\n",
      "Epoch 9, Loss: 925493.6873\n",
      "Epoch 10, Loss: 927113.7655\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9. Train Enhanced Regression Model\n",
    "# -----------------------------\n",
    "print(\"Training enhanced upvote regression model...\")\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        pred = model(batch_x)\n",
    "        loss = loss_fn(pred, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f8ce7-3340-4ccf-af4b-bc823826942b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
