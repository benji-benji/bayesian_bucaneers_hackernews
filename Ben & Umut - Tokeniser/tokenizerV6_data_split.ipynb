{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d364ea4-1943-452c-a286-73eb008527ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Arjuna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Hacker News data...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CBOW:\n\tMissing key(s) in state_dict: \"embeddings.weight\", \"linear.weight\", \"linear.bias\". \n\tUnexpected key(s) in state_dict: \"model_state_dict\", \"word_to_ix\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# cbow_loss_fn = nn.CrossEntropyLoss()\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# cbow_optimizer = optim.Adam(cbow_model.parameters(), lr=0.01)\u001b[39;00m\n\u001b[32m    112\u001b[39m state_dict = torch.load(\u001b[33m'\u001b[39m\u001b[33mumuts_cbow_full.pth\u001b[39m\u001b[33m'\u001b[39m, map_location=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43mcbow_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# # -----------------------------\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# # 4. Prepare CBOW training data\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# # -----------------------------\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# 6. Create title embeddings\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    153\u001b[39m title_embeddings = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:2153\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2148\u001b[39m         error_msgs.insert(\n\u001b[32m   2149\u001b[39m             \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m'\u001b[39m.format(\n\u001b[32m   2150\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[32m   2152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   2154\u001b[39m                        \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)))\n\u001b[32m   2155\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for CBOW:\n\tMissing key(s) in state_dict: \"embeddings.weight\", \"linear.weight\", \"linear.bias\". \n\tUnexpected key(s) in state_dict: \"model_state_dict\", \"word_to_ix\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import psycopg2\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Fetch data from PostgreSQL\n",
    "# -----------------------------\n",
    "# print(\"Fetching Hacker News data from PostgreSQL...\")\n",
    "# conn = psycopg2.connect(\"postgres://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\")\n",
    "# cur = conn.cursor()\n",
    "# cur.execute(\"SELECT title, url, score FROM hacker_news.items WHERE title IS NOT NULL AND score IS NOT NULL LIMIT 20000;\")\n",
    "# rows = cur.fetchall()\n",
    "# conn.close()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1a. 20% TESTITNG DATA LOAD\n",
    "# -----------------------------\n",
    "# print(\"Fetching Hacker News data...\")\n",
    "# conn = psycopg2.connect(\"postgres://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\")\n",
    "# cur = conn.cursor()\n",
    "# # the query below fetches 20% of the data for testing purposes\n",
    "# # it uses a hash function to ensure randomness in selection\n",
    "# # the condition (abs(hashtext(id::text)) % 100) < 20 is reproducable / will pick the same 20% of data every time\n",
    "# cur.execute('''SELECT title, url, score\n",
    "#             FROM hacker_news.items\n",
    "#             WHERE title IS NOT NULL AND score IS NOT NULL AND (abs(hashtext(id::text)) % 100) < 20;\n",
    "#             ''')\n",
    "\n",
    "\n",
    "# cur.execute('''SELECT i.title, i.url, i.score, u.id, u.karma \n",
    "#             FROM hacker_news.items AS i\n",
    "#             LEFT JOIN hacker_news.users AS u \n",
    "#                 ON i.by = u.id\n",
    "#             WHERE title IS NOT NULL AND score IS NOT NULL AND (abs(hashtext(i.id::text)) % 100) < 20;\n",
    "#             ''')\n",
    "# rows_test = cur.fetchall()\n",
    "# conn.close()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1b. 80% TRAINING & VALIDATION DATA LOAD LIMITED TO 100K ROWS & RECENT DATA\n",
    "# -----------------------------\n",
    "print(\"Fetching Hacker News data...\")\n",
    "conn = psycopg2.connect(\"postgres://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\")\n",
    "cur = conn.cursor()\n",
    "cur.execute('''SELECT title, url, score\n",
    "            FROM hacker_news.items\n",
    "            WHERE title IS NOT NULL \n",
    "                AND score IS NOT NULL \n",
    "                AND (abs(hashtext(id::text)) % 100) >= 20\n",
    "                AND EXTRACT(YEAR FROM time) >= 2023\n",
    "            LIMIT 100000;\n",
    "            ''')\n",
    "\n",
    "# cur.execute('''SELECT i.title, i.url, i.score, u.id, u.karma \n",
    "#             FROM hacker_news.items AS i\n",
    "#             LEFT JOIN hacker_news.users AS u \n",
    "#                 ON i.by = u.id\n",
    "#             WHERE title IS NOT NULL AND score IS NOT NULL AND (abs(hashtext(i.id::text)) % 100) >= 20\n",
    "#             LIMIT 100000;\n",
    "#             ''')\n",
    "rows = cur.fetchall()\n",
    "conn.close()\n",
    "\n",
    "titles, urls, scores = zip(*rows)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Preprocess titles\n",
    "# -----------------------------\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9 ]+', '', text)\n",
    "    return text.split()\n",
    "\n",
    "tokenized_titles = [preprocess(title) for title in titles]\n",
    "word_counts = Counter(word for title in tokenized_titles for word in title)\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. CBOW Model\n",
    "# -----------------------------\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).mean(dim=1)\n",
    "        return self.linear(embeds)\n",
    "\n",
    "embed_dim = 5\n",
    "cbow_model = CBOW(vocab_size, embed_dim)\n",
    "# cbow_loss_fn = nn.CrossEntropyLoss()\n",
    "# cbow_optimizer = optim.Adam(cbow_model.parameters(), lr=0.01)\n",
    "\n",
    "state_dict = torch.load('umuts_cbow_full.pth', map_location='cpu')\n",
    "cbow_model.load_state_dict(state_dict)\n",
    "\n",
    "# # -----------------------------\n",
    "# # 4. Prepare CBOW training data\n",
    "# # -----------------------------\n",
    "# context_size = 2\n",
    "# def make_cbow_data(tokenized_titles):\n",
    "#     data = []\n",
    "#     for tokens in tokenized_titles:\n",
    "#         if len(tokens) < context_size * 2 + 1:\n",
    "#             continue\n",
    "#         for i in range(context_size, len(tokens) - context_size):\n",
    "#             context = tokens[i - context_size:i] + tokens[i + 1:i + context_size + 1]\n",
    "#             target = tokens[i]\n",
    "#             data.append((\n",
    "#                 torch.tensor([word_to_ix[w] for w in context]),\n",
    "#                 torch.tensor(word_to_ix[target])\n",
    "#             ))\n",
    "#     return data\n",
    "\n",
    "# cbow_data = make_cbow_data(tokenized_titles)\n",
    "\n",
    "# # -----------------------------\n",
    "# # 5. Train CBOW model\n",
    "# # -----------------------------\n",
    "# print(\"Training CBOW model...\")\n",
    "# for epoch in range(10):\n",
    "#     total_loss = 0\n",
    "#     for context, target in cbow_data:\n",
    "#         output = cbow_model(context.unsqueeze(0))\n",
    "#         loss = cbow_loss_fn(output, target.unsqueeze(0))\n",
    "#         cbow_optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         cbow_optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     print(f\"CBOW Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Create title embeddings\n",
    "# -----------------------------\n",
    "title_embeddings = []\n",
    "valid_indices = []\n",
    "for i, tokens in enumerate(tokenized_titles):\n",
    "    token_ids = [word_to_ix[t] for t in tokens if t in word_to_ix]\n",
    "    if token_ids:\n",
    "        with torch.no_grad():\n",
    "            vectors = cbow_model.embeddings(torch.tensor(token_ids))\n",
    "            avg_vector = vectors.mean(dim=0)\n",
    "        title_embeddings.append(avg_vector)\n",
    "        valid_indices.append(i)\n",
    "\n",
    "X_title = torch.stack(title_embeddings)\n",
    "y = torch.tensor([scores[i] for i in valid_indices], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Process domain names\n",
    "# -----------------------------\n",
    "parsed_domains = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc or 'unknown'\n",
    "    except:\n",
    "        domain = 'unknown'\n",
    "    parsed_domains.append(domain)\n",
    "\n",
    "le = LabelEncoder()\n",
    "domain_ids = le.fit_transform(parsed_domains)\n",
    "domain_ids_tensor = torch.tensor(domain_ids, dtype=torch.long)[valid_indices]\n",
    "domain_vocab_size = len(le.classes_)\n",
    "domain_embed_dim = 3\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Regression Model\n",
    "# -----------------------------\n",
    "class UpvotePredictor(nn.Module):\n",
    "    def __init__(self, title_embed_dim, domain_vocab_size, domain_embed_dim):\n",
    "        super().__init__()\n",
    "        self.domain_embedding = nn.Embedding(domain_vocab_size, domain_embed_dim)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(title_embed_dim + domain_embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, title_embed, domain_id):\n",
    "        domain_vec = self.domain_embedding(domain_id)\n",
    "        x = torch.cat([title_embed, domain_vec], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "model = UpvotePredictor(embed_dim, domain_vocab_size, domain_embed_dim)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# -----------------------------\n",
    "# 9b. Train-validation split\n",
    "# -----------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose these are your full tensors covering all N examples:\n",
    "#   X_title          → (N, embed_dim) tensor of title embeddings\n",
    "#   domain_ids_tensor→ (N,)   tensor of domain IDs\n",
    "#   y                → (N,)   tensor of scores\n",
    "\n",
    "# First get numpy indices for splitting\n",
    "indices = list(range(len(y)))\n",
    "train_idx, val_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Index the tensors\n",
    "X_title_train      = X_title[train_idx]\n",
    "X_title_val        = X_title[val_idx]\n",
    "domain_ids_train   = domain_ids_tensor[train_idx]\n",
    "domain_ids_val     = domain_ids_tensor[val_idx]\n",
    "y_train            = y[train_idx]\n",
    "y_val              = y[val_idx]\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Dataset and Training\n",
    "# -----------------------------\n",
    "class HNDataset(Dataset):\n",
    "    def __init__(self, title_embeds, domain_ids, labels):\n",
    "        self.title_embeds = title_embeds\n",
    "        self.domain_ids = domain_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.title_embeds[idx], self.domain_ids[idx], self.labels[idx]\n",
    "\n",
    "# Create train‐only DataLoader\n",
    "train_ds = HNDataset(X_title_train, domain_ids_train, y_train)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Training regression model...\")\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for title_embed, domain_id, label in dataloader:\n",
    "        pred = model(title_embed, domain_id)\n",
    "        loss = loss_fn(pred, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Prediction Function\n",
    "# -----------------------------\n",
    "def predict_upvotes(title, url):\n",
    "    tokens = preprocess(title)\n",
    "    token_ids = [word_to_ix.get(t) for t in tokens if t in word_to_ix]\n",
    "    if not token_ids:\n",
    "        return None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vectors = cbow_model.embeddings(torch.tensor(token_ids))\n",
    "        avg_embed = vectors.mean(dim=0)\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc or 'unknown'\n",
    "    except:\n",
    "        domain = 'unknown'\n",
    "\n",
    "    try:\n",
    "        domain_id = le.transform([domain])[0]\n",
    "    except:\n",
    "        domain_id = 0  # fallback if unseen domain\n",
    "\n",
    "    domain_tensor = torch.tensor([domain_id], dtype=torch.long)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(avg_embed.unsqueeze(0), domain_tensor).item()\n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212299f9-b0ca-4bfd-890f-b6d04e0a50e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted upvotes for new post:\n",
      "Show HN: AI Hacker generates $1 billion → Predicted Upvotes: 7.63\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 11. Example\n",
    "# -----------------------------\n",
    "print(\"\\nPredicted upvotes for new post:\")\n",
    "\n",
    "example_title = \"what do you mean? okay\"\n",
    "example_title = \"Show HN: AI Hacker generates $1 billion\"\n",
    "example_url = \"https://openwall.com\"\n",
    "pred = predict_upvotes(example_title, example_url)\n",
    "print(f\"{example_title} → Predicted Upvotes: {pred:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648590bd-a9c2-42fa-8e1e-0be2fc62015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 12. Validate on held-out set\n",
    "# -----------------------------\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Make sure the model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Build a DataLoader over your validation split\n",
    "val_ds = HNDataset(\n",
    "    X_title_val,      # tensor of title embeddings for val\n",
    "    domain_ids_val,   # tensor of domain IDs for val\n",
    "    y_val             # tensor of true upvotes for val\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for title_embed, domain_id, target in val_loader:\n",
    "        pred = model(title_embed, domain_id).squeeze()\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_targets.extend(target.tolist())\n",
    "\n",
    "# Compute metrics\n",
    "mse = mean_squared_error(all_targets, all_preds)\n",
    "r2  = r2_score(all_targets, all_preds)\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "print(f\"Validation R² : {r2:.4f}\")\n",
    "\n",
    "# (Optional) Print first few examples\n",
    "print(\"\\nSome val-set predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\" Title: {val_titles[i]!r}\")\n",
    "    print(f\"  Actual upvotes: {all_targets[i]:.0f}\")\n",
    "    print(f\"  Predicted      : {all_preds[i]:.2f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
