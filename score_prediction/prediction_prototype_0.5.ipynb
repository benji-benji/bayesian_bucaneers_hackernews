{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d364ea4-1943-452c-a286-73eb008527ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import psycopg2\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6663d77-07c8-43e1-b0dd-c949720e9641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Hacker News data...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. LOAD 80% TRAINING & VALIDATION DATA - LIMITED TO 100K ROWS & RECENT DATA (>2023)\n",
    "# -----------------------------\n",
    "print(\"Fetching Hacker News data...\")\n",
    "conn = psycopg2.connect(\"postgres://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\")\n",
    "cur = conn.cursor()\n",
    "cur.execute('''SELECT i.title, i.url, i.score, u.id, u.karma \n",
    "            FROM hacker_news.items AS i\n",
    "            LEFT JOIN hacker_news.users AS u \n",
    "                ON i.by = u.id\n",
    "            WHERE title IS NOT NULL AND score IS NOT NULL AND (abs(hashtext(i.id::text)) % 100) >= 20\n",
    "            LIMIT 100000;\n",
    "            ''')\n",
    "rows = cur.fetchall()\n",
    "conn.close()\n",
    "\n",
    "titles, urls, scores, by, karmas = zip(*rows)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Preprocess titles\n",
    "# -----------------------------\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9 ]+', '', text)\n",
    "    return text.split()\n",
    "\n",
    "tokenized_titles = [preprocess(title) for title in titles]\n",
    "word_counts = Counter(word for title in tokenized_titles for word in title)\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "vocab_size = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6ea386-a5e0-4e65-ac3d-47ca5a3e141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Load Vocabulary\n",
    "# -----------------------------\n",
    "with open(\"vocab-w2-200titles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_to_ix = json.load(f)\n",
    "\n",
    "ix_to_word = {int(i): w for w, i in word_to_ix.items()}\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Load Pre-trained Embeddings\n",
    "# -----------------------------\n",
    "embed_dim = 300  \n",
    "embeddings = torch.load(\"embeddings-w2-200titles-300dim-10e.pt\", map_location='cpu')  # Shape: [vocab_size, embed_dim]\n",
    "assert embeddings.shape[0] == vocab_size, \"Vocab size mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d895e70-0534-4333-a4e0-bb870fee3183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / Val sizes: 78465 / 19617\n",
      "Training regression model...\n",
      "Epoch 0, Loss: 6253856.9892\n",
      "Epoch 1, Loss: 5771655.0751\n",
      "Epoch 2, Loss: 5683041.1936\n",
      "Epoch 3, Loss: 5661360.7728\n",
      "Epoch 4, Loss: 5641861.5457\n",
      "Epoch 5, Loss: 5641470.2598\n",
      "Epoch 6, Loss: 5610264.6250\n",
      "Epoch 7, Loss: 5562382.0188\n",
      "Epoch 8, Loss: 5527211.4172\n",
      "Epoch 9, Loss: 5501900.3622\n",
      "Epoch 10, Loss: 5476339.7773\n",
      "Epoch 11, Loss: 5429195.1267\n",
      "Epoch 12, Loss: 5395250.6327\n",
      "Epoch 13, Loss: 5386188.8530\n",
      "Epoch 14, Loss: 5346337.8243\n",
      "Epoch 15, Loss: 5323652.2957\n",
      "Epoch 16, Loss: 5271429.8059\n",
      "Epoch 17, Loss: 5237640.1682\n",
      "Epoch 18, Loss: 5180524.0599\n",
      "Epoch 19, Loss: 5205181.3213\n",
      "Epoch 20, Loss: 5172474.0960\n",
      "Epoch 21, Loss: 5108987.4730\n",
      "Epoch 22, Loss: 5815037.4241\n",
      "Epoch 23, Loss: 4988263.6972\n",
      "Epoch 24, Loss: 5135064.5453\n",
      "Epoch 25, Loss: 5022083.6730\n",
      "Epoch 26, Loss: 4915307.4249\n",
      "Epoch 27, Loss: 4884120.4278\n",
      "Epoch 28, Loss: 4835498.2604\n",
      "Epoch 29, Loss: 4786737.5684\n",
      "Epoch 30, Loss: 4783229.3823\n",
      "Epoch 31, Loss: 4753886.8878\n",
      "Epoch 32, Loss: 4675765.1929\n",
      "Epoch 33, Loss: 4706692.0293\n",
      "Epoch 34, Loss: 4783167.1295\n",
      "Epoch 35, Loss: 4588500.1043\n",
      "Epoch 36, Loss: 4629455.3096\n",
      "Epoch 37, Loss: 4671714.7142\n",
      "Epoch 38, Loss: 4671985.4293\n",
      "Epoch 39, Loss: 4984044.1406\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4. CBOW Model\n",
    "# -----------------------------\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).mean(dim=1)\n",
    "        return self.linear(embeds)\n",
    "\n",
    "cbow_model = CBOW(vocab_size, embed_dim)\n",
    "cbow_model.embeddings.weight.data.copy_(embeddings)\n",
    "cbow_model.embeddings.weight.requires_grad = False  \n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Create title embeddings\n",
    "# -----------------------------\n",
    "title_embeddings = []\n",
    "valid_indices = []\n",
    "for i, tokens in enumerate(tokenized_titles):\n",
    "    token_ids = [word_to_ix[t] for t in tokens if t in word_to_ix]\n",
    "    if token_ids:\n",
    "        with torch.no_grad():\n",
    "            vectors = cbow_model.embeddings(torch.tensor(token_ids))\n",
    "            avg_vector = vectors.mean(dim=0)\n",
    "        title_embeddings.append(avg_vector)\n",
    "        valid_indices.append(i)\n",
    "\n",
    "X_title = torch.stack(title_embeddings)\n",
    "y = torch.tensor([scores[i] for i in valid_indices], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "\n",
    "karmas_tensor = torch.tensor([karmas[i] for i in valid_indices], dtype=torch.float32).unsqueeze(1)  # NEW\n",
    "user_ids = [by[i] for i in valid_indices]  # NEW\n",
    "user_karma_lookup = {user_ids[i]: karmas_tensor[i].item() for i in range(len(user_ids))}  # NEW\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Process domain names\n",
    "# -----------------------------\n",
    "parsed_domains = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc or 'unknown'\n",
    "    except:\n",
    "        domain = 'unknown'\n",
    "    parsed_domains.append(domain)\n",
    "\n",
    "le = LabelEncoder()\n",
    "domain_ids = le.fit_transform(parsed_domains)\n",
    "domain_ids_tensor = torch.tensor(domain_ids, dtype=torch.long)[valid_indices]\n",
    "domain_vocab_size = len(le.classes_)\n",
    "domain_embed_dim = 3\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Regression Model\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "class UpvotePredictor(nn.Module):\n",
    "    def __init__(self, title_embed_dim, domain_vocab_size, domain_embed_dim):\n",
    "        super().__init__()\n",
    "        self.domain_embedding = nn.Embedding(domain_vocab_size, domain_embed_dim)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(title_embed_dim + domain_embed_dim + 1, 128),  # CHANGED\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, title_embed, domain_id, karma):  # CHANGED\n",
    "        domain_vec = self.domain_embedding(domain_id)\n",
    "        x = torch.cat([title_embed, domain_vec, karma], dim=1)  # CHANGED\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "model = UpvotePredictor(embed_dim, domain_vocab_size, domain_embed_dim)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8b. Train-validation split\n",
    "# -----------------------------\n",
    "\n",
    "# Assume you already have these full N-length tensors from V8 preprocessing:\n",
    "#   X_title           â€“ (N, title_embed_dim) torch.Tensor  \n",
    "#   domain_ids_tensor â€“ (N,)               torch.LongTensor\n",
    "#   karma_tensor      â€“ (N,)               torch.FloatTensor\n",
    "#   y                 â€“ (N,)               torch.FloatTensor (scores)\n",
    "\n",
    "# Build an array of row indices\n",
    "all_idx = list(range(y.size(0)))\n",
    "\n",
    "# Split indices into 80% train / 20% val\n",
    "train_idx, val_idx = train_test_split(\n",
    "    all_idx,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Index into each tensor\n",
    "X_title_train      = X_title[train_idx]\n",
    "X_title_val        = X_title[val_idx]\n",
    "\n",
    "domain_ids_train   = domain_ids_tensor[train_idx]\n",
    "domain_ids_val     = domain_ids_tensor[val_idx]\n",
    "\n",
    "karmas_train        = karmas_tensor[train_idx]\n",
    "karmas_val          = karmas_tensor[val_idx]\n",
    "\n",
    "y_train            = y[train_idx]\n",
    "y_val              = y[val_idx]\n",
    "\n",
    "# Print sizes of training and validation sets\n",
    "print(f\"Train / Val sizes: {len(train_idx)} / {len(val_idx)}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Dataset and Training\n",
    "# -----------------------------\n",
    "class HNDataset(Dataset):\n",
    "    def __init__(self, title_embeds, domain_ids, karmas, labels):  # CHANGED\n",
    "        self.title_embeds = title_embeds\n",
    "        self.domain_ids = domain_ids\n",
    "        self.karmas = karmas  # NEW\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.title_embeds[idx], self.domain_ids[idx], self.karmas[idx], self.labels[idx]  # CHANGED\n",
    "\n",
    "dataset = HNDataset(X_title_train, domain_ids_train, karmas_train, y_train)  # CHANGED\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Training regression model...\")\n",
    "for epoch in range(40):\n",
    "    total_loss = 0\n",
    "    for title_embed, domain_id, karma, label in dataloader:  # CHANGED\n",
    "        pred = model(title_embed, domain_id, karma)  # CHANGED\n",
    "        loss = loss_fn(pred, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Prediction Function\n",
    "# -----------------------------\n",
    "def predict_upvotes(title, url, user_id):  # CHANGED\n",
    "    tokens = preprocess(title)\n",
    "    token_ids = [word_to_ix.get(t) for t in tokens if t in word_to_ix]\n",
    "    token_ids = [i for i in token_ids if i is not None]\n",
    "    if not token_ids:\n",
    "        return None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vectors = cbow_model.embeddings(torch.tensor(token_ids))\n",
    "        avg_embed = vectors.mean(dim=0)\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc or 'unknown'\n",
    "    except:\n",
    "        domain = 'unknown'\n",
    "\n",
    "    try:\n",
    "        domain_id = le.transform([domain])[0]\n",
    "    except:\n",
    "        domain_id = 0  # fallback\n",
    "\n",
    "    domain_tensor = torch.tensor([domain_id], dtype=torch.long)\n",
    "    karma_value = user_karma_lookup.get(user_id, 0)  # NEW\n",
    "    karma_tensor = torch.tensor([[karma_value]], dtype=torch.float32)  # NEW\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(avg_embed.unsqueeze(0), domain_tensor, karma_tensor).item()  # CHANGED\n",
    "    return max(prediction, 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "212299f9-b0ca-4bfd-890f-b6d04e0a50e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted upvotes for new post:\n",
      "Show HN: AI Hacker generates $1 billion â†’ Predicted Upvotes: 1.68\n",
      "Eat. Pray. Deploy. Blame the data. ðŸ˜Ž Â© Bayesian Buccaneers\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 11. Example\n",
    "# -----------------------------\n",
    "print(\"\\nPredicted upvotes for new post:\")\n",
    "example_title = \"Generates $1 billion today!\"\n",
    "example_title = \"Close Votes Are A Feature, Not A Bug\"\n",
    "example_title = \"what do you mean? okay\"\n",
    "example_title = \"Show HN: AI Hacker generates $1 billion\"\n",
    "example_url = \"https://openai.com\"\n",
    "user_id = \"hackerl33t\"\n",
    "pred = predict_upvotes(example_title, example_url, user_id)\n",
    "print(f\"{example_title} â†’ Predicted Upvotes: {pred:.2f}\")\n",
    "print(\"Eat. Pray. Deploy. Blame the data. ðŸ˜Ž Â© Bayesian Buccaneers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f85da00a-1a0b-479a-9b23-e1002572bb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 2391.8670\n",
      "Validation RÂ² : -0.2494\n",
      "\n",
      "Sample predictions vs. actuals:\n",
      "  Predicted: 4.66  |  Actual: 2\n",
      "  Predicted: 7.09  |  Actual: 3\n",
      "  Predicted: 4.21  |  Actual: 1\n",
      "  Predicted: 0.52  |  Actual: 1\n",
      "  Predicted: 2.04  |  Actual: 3\n",
      "  Predicted: 15.76  |  Actual: 1\n",
      "  Predicted: -0.75  |  Actual: 1\n",
      "  Predicted: 15.76  |  Actual: 1\n",
      "  Predicted: 5.64  |  Actual: 1\n",
      "  Predicted: 14.48  |  Actual: 1\n",
      "  Predicted: 11.57  |  Actual: 1\n",
      "  Predicted: 6.90  |  Actual: 3\n",
      "  Predicted: 18.33  |  Actual: 2\n",
      "  Predicted: 5.72  |  Actual: 1\n",
      "  Predicted: 1.57  |  Actual: 1\n",
      "  Predicted: -1.59  |  Actual: 1\n",
      "  Predicted: 2.12  |  Actual: 1\n",
      "  Predicted: 93.37  |  Actual: 1\n",
      "  Predicted: 5.82  |  Actual: 1\n",
      "  Predicted: 15.76  |  Actual: 3\n",
      "  Predicted: 2.69  |  Actual: 2\n",
      "  Predicted: 12.02  |  Actual: 1\n",
      "  Predicted: 4.79  |  Actual: 1\n",
      "  Predicted: 10.84  |  Actual: 2\n",
      "  Predicted: 15.76  |  Actual: 19\n",
      "  Predicted: 15.76  |  Actual: 3\n",
      "  Predicted: 11.06  |  Actual: 3\n",
      "  Predicted: 3.32  |  Actual: 2\n",
      "  Predicted: 0.67  |  Actual: 3\n",
      "  Predicted: 13.14  |  Actual: 1\n",
      "  Predicted: 16.59  |  Actual: 1\n",
      "  Predicted: 6.61  |  Actual: 3\n",
      "  Predicted: 27.95  |  Actual: 8\n",
      "  Predicted: 15.76  |  Actual: 2\n",
      "  Predicted: 7.15  |  Actual: 2\n",
      "  Predicted: 5.04  |  Actual: 2\n",
      "  Predicted: -1.31  |  Actual: 2\n",
      "  Predicted: 8.23  |  Actual: 1\n",
      "  Predicted: 15.76  |  Actual: 3\n",
      "  Predicted: 15.76  |  Actual: 15\n",
      "  Predicted: 4.41  |  Actual: 1\n",
      "  Predicted: 5.19  |  Actual: 1\n",
      "  Predicted: 44.91  |  Actual: 1\n",
      "  Predicted: -0.39  |  Actual: 1\n",
      "  Predicted: 6.58  |  Actual: 1\n",
      "  Predicted: 0.49  |  Actual: 1\n",
      "  Predicted: 44.06  |  Actual: 11\n",
      "  Predicted: 16.23  |  Actual: 2\n",
      "  Predicted: 101.77  |  Actual: 83\n",
      "  Predicted: 6.73  |  Actual: 8\n",
      "  Predicted: 15.76  |  Actual: 154\n",
      "  Predicted: 25.01  |  Actual: 5\n",
      "  Predicted: 5.23  |  Actual: 1\n",
      "  Predicted: 1.84  |  Actual: 2\n",
      "  Predicted: 2.66  |  Actual: 3\n",
      "  Predicted: 4.03  |  Actual: 1\n",
      "  Predicted: 15.76  |  Actual: 18\n",
      "  Predicted: -2.09  |  Actual: 1\n",
      "  Predicted: 15.76  |  Actual: 3\n",
      "  Predicted: 7.19  |  Actual: 5\n",
      "  Predicted: 13.88  |  Actual: 2\n",
      "  Predicted: 22.80  |  Actual: 1\n",
      "  Predicted: 15.76  |  Actual: 4\n",
      "  Predicted: 15.76  |  Actual: 7\n",
      "  Predicted: 0.57  |  Actual: 1\n",
      "  Predicted: -1.77  |  Actual: 1\n",
      "  Predicted: -4.64  |  Actual: 1\n",
      "  Predicted: 5.84  |  Actual: 1\n",
      "  Predicted: -1.19  |  Actual: 3\n",
      "  Predicted: 17.39  |  Actual: 7\n",
      "  Predicted: 5.73  |  Actual: 1\n",
      "  Predicted: 0.98  |  Actual: 1\n",
      "  Predicted: 2.36  |  Actual: 2\n",
      "  Predicted: 13.22  |  Actual: 12\n",
      "  Predicted: 14.89  |  Actual: 13\n",
      "  Predicted: 15.76  |  Actual: 1\n",
      "  Predicted: 39.66  |  Actual: 69\n",
      "  Predicted: 15.76  |  Actual: 1\n",
      "  Predicted: 0.90  |  Actual: 1\n",
      "  Predicted: 7.16  |  Actual: 1\n",
      "  Predicted: 15.76  |  Actual: 3\n",
      "  Predicted: 27.14  |  Actual: 3\n",
      "  Predicted: 3.26  |  Actual: 1\n",
      "  Predicted: 3.32  |  Actual: 3\n",
      "  Predicted: 6.60  |  Actual: 1\n",
      "  Predicted: 160.92  |  Actual: 3\n",
      "  Predicted: 15.76  |  Actual: 5\n",
      "  Predicted: 3.27  |  Actual: 1\n",
      "  Predicted: 1.17  |  Actual: 1\n",
      "  Predicted: 4.78  |  Actual: 13\n",
      "  Predicted: 15.76  |  Actual: 30\n",
      "  Predicted: 11.01  |  Actual: 2\n",
      "  Predicted: 15.76  |  Actual: 1\n",
      "  Predicted: 8.55  |  Actual: 4\n",
      "  Predicted: 12.54  |  Actual: 4\n",
      "  Predicted: 9.02  |  Actual: 1\n",
      "  Predicted: 15.76  |  Actual: 117\n",
      "  Predicted: 8.18  |  Actual: 1\n",
      "  Predicted: -2.78  |  Actual: 1\n",
      "  Predicted: -5.74  |  Actual: 5\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 12. Validation on held-out split\n",
    "# -----------------------------\n",
    "\n",
    "# Build a DataLoader for the validation set\n",
    "dataset_val = HNDataset(\n",
    "    X_title_val,\n",
    "    domain_ids_val,\n",
    "    karmas_val,\n",
    "    y_val\n",
    ")\n",
    "val_loader = DataLoader(dataset_val, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for title_emb, dom_id, krm, target in val_loader:\n",
    "        pred = model(title_emb, dom_id, krm).squeeze()\n",
    "        pred = pred.view(-1)\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_targets.extend(target.tolist())\n",
    "\n",
    "# Flatten any singleton lists in all_targets (and preds, just in case)\n",
    "all_preds_flat   = [p[0] if isinstance(p, list) else p   for p in all_preds]\n",
    "all_targets_flat = [t[0] if isinstance(t, list) else t   for t in all_targets]\n",
    "\n",
    "# Compute metrics\n",
    "mse = mean_squared_error(all_targets, all_preds)\n",
    "r2  = r2_score(all_targets, all_preds)\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "print(f\"Validation RÂ² : {r2:.4f}\")\n",
    "\n",
    "\n",
    "# Optional: inspect a few examples\n",
    "print(\"\\nSample predictions vs. actuals:\")\n",
    "for i in range(100):\n",
    "    print(f\"  Predicted: {all_preds_flat[i]:.2f}  |  Actual: {all_targets_flat[i]:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aa9bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
