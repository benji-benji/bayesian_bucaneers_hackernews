{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d364ea4-1943-452c-a286-73eb008527ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tom\\miniconda3\\envs\\MLX\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import psycopg2\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "705f5bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Hacker News data from PostgreSQL...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 1. Fetch data from PostgreSQL\n",
    "# -----------------------------\n",
    "print(\"Fetching Hacker News data from PostgreSQL...\")\n",
    "conn = psycopg2.connect(\"postgres://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT title, url, score FROM hacker_news.items WHERE title IS NOT NULL AND score IS NOT NULL LIMIT 20000;\")\n",
    "rows = cur.fetchall()\n",
    "conn.close()\n",
    "\n",
    "titles, urls, scores = zip(*rows)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Preprocess titles\n",
    "# -----------------------------\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9 ]+', '', text)\n",
    "    return text.split()\n",
    "\n",
    "tokenized_titles = [preprocess(title) for title in titles]\n",
    "word_counts = Counter(word for title in tokenized_titles for word in title)\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "vocab_size = len(word_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39b65f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 3. Load Vocabulary\n",
    "# -----------------------------\n",
    "with open(\"vocab-w2-200titles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_to_ix = json.load(f)\n",
    "\n",
    "ix_to_word = {int(i): w for w, i in word_to_ix.items()}\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Load Pre-trained Embeddings\n",
    "# -----------------------------\n",
    "embed_dim = 300  \n",
    "embeddings = torch.load(\"embeddings-w2-200titles-300dim-10e.pt\", map_location='cpu')  # Shape: [vocab_size, embed_dim]\n",
    "assert embeddings.shape[0] == vocab_size, \"Vocab size mismatch!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a4edf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training regression model...\n",
      "Epoch 0, Loss: 3505053.8513\n",
      "Epoch 1, Loss: 3465769.0129\n",
      "Epoch 2, Loss: 3422931.6892\n",
      "Epoch 3, Loss: 3379705.4300\n",
      "Epoch 4, Loss: 3305101.5542\n",
      "Epoch 5, Loss: 3220818.2889\n",
      "Epoch 6, Loss: 3120895.7736\n",
      "Epoch 7, Loss: 3012050.1520\n",
      "Epoch 8, Loss: 2894646.6732\n",
      "Epoch 9, Loss: 2759575.0025\n",
      "Epoch 10, Loss: 2607334.1094\n",
      "Epoch 11, Loss: 2423618.8444\n",
      "Epoch 12, Loss: 2194236.4211\n",
      "Epoch 13, Loss: 1999496.9757\n",
      "Epoch 14, Loss: 1737605.4961\n",
      "Epoch 15, Loss: 1517183.8288\n",
      "Epoch 16, Loss: 1334066.1755\n",
      "Epoch 17, Loss: 1275192.3249\n",
      "Epoch 18, Loss: 1076709.5625\n",
      "Epoch 19, Loss: 936962.6045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. CBOW Model\n",
    "# -----------------------------\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).mean(dim=1)\n",
    "        return self.linear(embeds)\n",
    "\n",
    "cbow_model = CBOW(vocab_size, embed_dim)\n",
    "cbow_model.embeddings.weight.data.copy_(embeddings)\n",
    "cbow_model.embeddings.weight.requires_grad = False  \n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Create title embeddings\n",
    "# -----------------------------\n",
    "title_embeddings = []\n",
    "valid_indices = []\n",
    "for i, tokens in enumerate(tokenized_titles):\n",
    "    token_ids = [word_to_ix[t] for t in tokens if t in word_to_ix]\n",
    "    if token_ids:\n",
    "        with torch.no_grad():\n",
    "            vectors = cbow_model.embeddings(torch.tensor(token_ids))\n",
    "            avg_vector = vectors.mean(dim=0)\n",
    "        title_embeddings.append(avg_vector)\n",
    "        valid_indices.append(i)\n",
    "\n",
    "X_title = torch.stack(title_embeddings)\n",
    "y = torch.tensor([scores[i] for i in valid_indices], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Process domain names\n",
    "# -----------------------------\n",
    "parsed_domains = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc or 'unknown'\n",
    "    except:\n",
    "        domain = 'unknown'\n",
    "    parsed_domains.append(domain)\n",
    "\n",
    "le = LabelEncoder()\n",
    "domain_ids = le.fit_transform(parsed_domains)\n",
    "domain_ids_tensor = torch.tensor(domain_ids, dtype=torch.long)[valid_indices]\n",
    "domain_vocab_size = len(le.classes_)\n",
    "domain_embed_dim = 3\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Regression Model\n",
    "# -----------------------------\n",
    "class UpvotePredictor(nn.Module):\n",
    "    def __init__(self, title_embed_dim, domain_vocab_size, domain_embed_dim):\n",
    "        super().__init__()\n",
    "        self.domain_embedding = nn.Embedding(domain_vocab_size, domain_embed_dim)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(title_embed_dim + domain_embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, title_embed, domain_id):\n",
    "        domain_vec = self.domain_embedding(domain_id)\n",
    "        x = torch.cat([title_embed, domain_vec], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "model = UpvotePredictor(embed_dim, domain_vocab_size, domain_embed_dim)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Dataset and Training\n",
    "# -----------------------------\n",
    "class HNDataset(Dataset):\n",
    "    def __init__(self, title_embeds, domain_ids, labels):\n",
    "        self.title_embeds = title_embeds\n",
    "        self.domain_ids = domain_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.title_embeds[idx], self.domain_ids[idx], self.labels[idx]\n",
    "\n",
    "dataset = HNDataset(X_title, domain_ids_tensor, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Training regression model...\")\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for title_embed, domain_id, label in dataloader:\n",
    "        pred = model(title_embed, domain_id)\n",
    "        loss = loss_fn(pred, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Prediction Function\n",
    "# -----------------------------\n",
    "def predict_upvotes(title, url):\n",
    "    tokens = preprocess(title)\n",
    "    token_ids = [word_to_ix.get(t) for t in tokens if t in word_to_ix]\n",
    "    if not token_ids:\n",
    "        return None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vectors = cbow_model.embeddings(torch.tensor(token_ids))\n",
    "        avg_embed = vectors.mean(dim=0)\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc or 'unknown'\n",
    "    except:\n",
    "        domain = 'unknown'\n",
    "\n",
    "    try:\n",
    "        domain_id = le.transform([domain])[0]\n",
    "    except:\n",
    "        domain_id = 0  # fallback if unseen domain\n",
    "\n",
    "    domain_tensor = torch.tensor([domain_id], dtype=torch.long)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(avg_embed.unsqueeze(0), domain_tensor).item()\n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "212299f9-b0ca-4bfd-890f-b6d04e0a50e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted upvotes for new post:\n",
      "The librarian immediately attempts to sell you a vuvuzela → Predicted Upvotes: 4.75\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 11. Example\n",
    "# -----------------------------\n",
    "print(\"\\nPredicted upvotes for new post:\")\n",
    "\n",
    "example_title = \"what do you mean? okay\"\n",
    "example_title = \"The librarian immediately attempts to sell you a vuvuzela\"\n",
    "example_url = \"kaveland.no\"\n",
    "pred = predict_upvotes(example_title, example_url)\n",
    "print(f\"{example_title} → Predicted Upvotes: {pred:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
